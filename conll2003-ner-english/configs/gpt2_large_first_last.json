{
    "description": "GPT-2 large with First and Last Subword Pooling and Scalar Mix",
    "embeddings": ["gpt2-large"],
    "layers": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],
    "batch_size": 16,
    "hidden_size": 256,
    "max_epochs": 500,
    "embeddings_storage_mode": "gpu",
    "pooling_operation": "first_last",
    "use_crf": true,
    "use_scalar_mix": true,
    "train_with_dev": false
}